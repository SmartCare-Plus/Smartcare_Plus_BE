"""
SMARTCARE+ Guardian Service - Video Classifier

Owner: Madhushani
Deep learning video classification for activity and fall detection.
Uses trained EfficientNetV2-S + LSTM model.
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import numpy as np
import cv2

# Lazy load TensorFlow to avoid startup overhead
_tf = None
_model = None

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Model paths
MODEL_DIR = Path(__file__).parent.parent.parent / "ml_models" / "guardian"
MODEL_FILE = "guardian_phase1_20260102_233146.keras"
WEIGHTS_FILE = "guardian_weights_20260103T071820Z.weights.h5"
SAVEDMODEL_DIR = "guardian_savedmodel_20260103T071820Z"  # SavedModel format (most reliable)
CLASS_MAPPING_FILE = "class_mapping_20260102_233146.json"

# Video processing settings (must match training config)
NUM_FRAMES = 32
FRAME_HEIGHT = 224
FRAME_WIDTH = 224
NUM_CLASSES = 5


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENUMS AND DATA CLASSES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VideoActivityType(Enum):
    """Activity types from video classification model."""
    ADL = "adl"  # Activities of Daily Living
    ARTHRITIS_GAIT = "arthritis_gait"
    FALL = "fall"
    GOOD_GAIT = "good_gait"
    TUG = "tug"  # Timed Up and Go test
    UNKNOWN = "unknown"


@dataclass
class VideoClassificationResult:
    """Result from video classification."""
    activity: VideoActivityType
    confidence: float
    all_probabilities: Dict[str, float]
    is_fall_detected: bool
    is_gait_abnormal: bool
    processing_time_ms: float
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "activity": self.activity.value,
            "confidence": round(self.confidence, 3),
            "probabilities": {k: round(v, 3) for k, v in self.all_probabilities.items()},
            "fall_detected": self.is_fall_detected,
            "gait_abnormal": self.is_gait_abnormal,
            "processing_time_ms": round(self.processing_time_ms, 1),
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VIDEO CLASSIFIER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VideoClassifier:
    """
    Deep learning video classifier for activity recognition.
    
    Uses EfficientNetV2-S + LSTM model trained on fall/gait dataset.
    Processes video clips and returns activity predictions.
    """
    
    def __init__(self, model_path: Optional[str] = None, class_mapping_path: Optional[str] = None):
        """
        Initialize the video classifier.
        
        Args:
            model_path: Path to .keras model file (uses default if None)
            class_mapping_path: Path to class mapping JSON (uses default if None)
        """
        self.model = None
        self.class_mapping: Dict[int, str] = {}
        self.class_to_idx: Dict[str, int] = {}
        self.is_loaded = False
        
        # Set paths
        self.model_path = Path(model_path) if model_path else MODEL_DIR / MODEL_FILE
        self.weights_path = MODEL_DIR / WEIGHTS_FILE
        self.savedmodel_path = MODEL_DIR / SAVEDMODEL_DIR  # SavedModel format
        self.class_mapping_path = Path(class_mapping_path) if class_mapping_path else MODEL_DIR / CLASS_MAPPING_FILE
        
        # Frame buffer for real-time processing
        self.frame_buffer: List[np.ndarray] = []
        self.max_buffer_size = NUM_FRAMES
        
        logger.info(f"VideoClassifier initialized")
        logger.info(f"  SavedModel: {self.savedmodel_path}")
        logger.info(f"  Keras Model: {self.model_path}")
        logger.info(f"  Weights: {self.weights_path}")
        logger.info(f"  Classes: {self.class_mapping_path}")
    
    def _build_model_architecture(self, tf):
        """
        Rebuild the exact model architecture used during training.
        Must match train_guardian_phase1_only.py exactly.
        
        Architecture: EfficientNetV2-S (frozen) + LSTM + Dense
        """
        from tensorflow.keras import layers, Model
        
        # Input layer: (batch, frames, height, width, channels)
        video_input = layers.Input(shape=(NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3), name='video_input')
        
        # Load pre-trained EfficientNetV2-S (frozen for feature extraction)
        # NOTE: First run downloads ~80MB ImageNet weights - this is cached for future use
        logger.info("Loading EfficientNetV2-S backbone (may download ~80MB on first run)...")
        backbone = tf.keras.applications.EfficientNetV2S(
            include_top=False,
            weights='imagenet',
            pooling='avg',
            input_shape=(FRAME_HEIGHT, FRAME_WIDTH, 3)
        )
        backbone.trainable = False  # Frozen during Phase 1
        logger.info("EfficientNetV2-S backbone loaded")
        
        # Apply backbone to each frame using TimeDistributed
        x = layers.TimeDistributed(backbone, name='frame_features')(video_input)
        # Output: (batch, frames, 1280)
        
        # Temporal modeling with LSTM
        x = layers.LSTM(256, return_sequences=False, dropout=0.3, name='temporal_lstm')(x)
        # Output: (batch, 256)
        
        # Classification head
        x = layers.Dense(128, activation='relu', name='fc1')(x)
        x = layers.Dropout(0.4, name='dropout1')(x)
        x = layers.Dense(64, activation='relu', name='fc2')(x)
        x = layers.Dropout(0.3, name='dropout2')(x)
        
        # Output layer
        outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)
        
        model = Model(inputs=video_input, outputs=outputs, name='GuardianVideoClassifier')
        return model
    
    def load_model(self) -> bool:
        """
        Load the Keras model and class mapping.
        Tries multiple loading strategies in order of reliability:
        1. Load SavedModel directory (most reliable, includes full model)
        2. Build architecture and load weights
        3. Load .keras file directly
        4. Fall back to rule-based classifier
        
        Returns:
            True if loaded successfully, False otherwise.
        """
        global _tf, _model
        
        if self.is_loaded:
            return True
        
        try:
            # Load class mapping first (always needed)
            if self.class_mapping_path.exists():
                with open(self.class_mapping_path, 'r') as f:
                    raw_mapping = json.load(f)
                    # Convert string keys to int
                    self.class_mapping = {int(k): v for k, v in raw_mapping.items()}
                    self.class_to_idx = {v: int(k) for k, v in raw_mapping.items()}
                logger.info(f"Loaded class mapping: {self.class_mapping}")
            else:
                logger.warning(f"Class mapping not found: {self.class_mapping_path}")
                # Default mapping
                self.class_mapping = {
                    0: "adl",
                    1: "arthritis_gait",
                    2: "fall",
                    3: "good_gait",
                    4: "tug"
                }
                self.class_to_idx = {v: k for k, v in self.class_mapping.items()}
            
            # Try to import TensorFlow
            if _tf is None:
                logger.info("Loading TensorFlow...")
                import tensorflow as tf
                _tf = tf
                
                # Suppress verbose logging
                tf.get_logger().setLevel('ERROR')
                
                # Configure GPU memory growth
                gpus = tf.config.list_physical_devices('GPU')
                if gpus:
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    logger.info(f"GPU memory growth enabled for {len(gpus)} GPU(s)")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STRATEGY 1: Try loading SavedModel using TFSMLayer (Keras 3 compatible)
            # In Keras 3, legacy SavedModel must be loaded via TFSMLayer
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.savedmodel_path.exists() and self.savedmodel_path.is_dir():
                try:
                    logger.info(f"Loading SavedModel via TFSMLayer from: {self.savedmodel_path}")
                    
                    # Create a wrapper model using TFSMLayer for Keras 3 compatibility
                    tfsm_layer = _tf.keras.layers.TFSMLayer(
                        str(self.savedmodel_path), 
                        call_endpoint='serving_default'
                    )
                    
                    # Build a functional model wrapper around the TFSMLayer
                    # Input shape: (batch, frames, height, width, channels)
                    video_input = _tf.keras.Input(shape=(NUM_FRAMES, FRAME_HEIGHT, FRAME_WIDTH, 3), name='video_input')
                    outputs = tfsm_layer(video_input)
                    
                    # Handle different output formats from SavedModel
                    if isinstance(outputs, dict):
                        # SavedModel returns dict with output tensor name as key
                        output_key = list(outputs.keys())[0]
                        outputs = outputs[output_key]
                        logger.info(f"  SavedModel output key: {output_key}")
                    
                    self.model = _tf.keras.Model(inputs=video_input, outputs=outputs, name='GuardianVideoClassifier')
                    self._tfsm_layer = tfsm_layer  # Keep reference
                    _model = self.model
                    self._use_fallback = False
                    
                    logger.info(f"âœ… Model loaded successfully from SavedModel via TFSMLayer!")
                    logger.info(f"  Input shape: {self.model.input_shape}")
                    logger.info(f"  Output shape: {self.model.output_shape}")
                    
                    self.is_loaded = True
                    return True
                    
                except Exception as savedmodel_error:
                    logger.warning(f"Could not load SavedModel via TFSMLayer: {savedmodel_error}")
                    import traceback
                    traceback.print_exc()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STRATEGY 2: Try loading .keras file directly (Keras 3 native format)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.model_path.exists():
                try:
                    logger.info(f"Loading model from: {self.model_path}")
                    self.model = _tf.keras.models.load_model(str(self.model_path), compile=False)
                    _model = self.model
                    self._use_fallback = False
                    
                    logger.info(f"âœ… Model loaded successfully from .keras!")
                    logger.info(f"  Input shape: {self.model.input_shape}")
                    logger.info(f"  Output shape: {self.model.output_shape}")
                    logger.info(f"  Parameters: {self.model.count_params():,}")
                    
                    self.is_loaded = True
                    return True
                    
                except Exception as keras_error:
                    logger.warning(f"Could not load .keras model: {keras_error}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STRATEGY 3: Try building architecture and loading weights
            # Note: Requires downloading EfficientNetV2 backbone (~80MB)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.weights_path.exists():
                try:
                    logger.info(f"Building model architecture...")
                    self.model = self._build_model_architecture(_tf)
                    
                    logger.info(f"Loading weights from: {self.weights_path}")
                    self.model.load_weights(str(self.weights_path))
                    _model = self.model
                    self._use_fallback = False
                    
                    logger.info(f"âœ… Model loaded successfully from weights!")
                    logger.info(f"  Input shape: {self.model.input_shape}")
                    logger.info(f"  Output shape: {self.model.output_shape}")
                    logger.info(f"  Parameters: {self.model.count_params():,}")
                    
                    self.is_loaded = True
                    return True
                    
                except Exception as weights_error:
                    logger.warning(f"Could not load from weights: {weights_error}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STRATEGY 4: Fall back to rule-based classifier
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.warning("No model files found or loadable")
            logger.warning("Using rule-based fallback classifier")
            self._use_fallback = True
            self.is_loaded = True
            return True
            
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def preprocess_frames(self, frames: List[np.ndarray]) -> np.ndarray:
        """
        Preprocess frames for model input.
        
        Args:
            frames: List of BGR frames (OpenCV format)
            
        Returns:
            Preprocessed array of shape (1, NUM_FRAMES, HEIGHT, WIDTH, 3)
        """
        processed = []
        
        for frame in frames:
            # Convert BGR to RGB
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Resize
            frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))
            
            # Normalize to [0, 1]
            frame = frame.astype(np.float32) / 255.0
            
            processed.append(frame)
        
        # Pad or trim to NUM_FRAMES
        while len(processed) < NUM_FRAMES:
            if processed:
                processed.append(processed[-1].copy())
            else:
                processed.append(np.zeros((FRAME_HEIGHT, FRAME_WIDTH, 3), dtype=np.float32))
        
        processed = processed[:NUM_FRAMES]
        
        # Stack and add batch dimension
        frames_array = np.stack(processed, axis=0)
        return np.expand_dims(frames_array, axis=0)  # Shape: (1, 32, 224, 224, 3)
    
    def classify_frames(self, frames: List[np.ndarray]) -> VideoClassificationResult:
        """
        Classify a sequence of video frames.
        
        Args:
            frames: List of video frames (at least 8, ideally 32)
            
        Returns:
            VideoClassificationResult with activity prediction
        """
        import time
        start_time = time.time()
        
        # Ensure model is loaded
        if not self.is_loaded:
            if not self.load_model():
                return VideoClassificationResult(
                    activity=VideoActivityType.UNKNOWN,
                    confidence=0.0,
                    all_probabilities={},
                    is_fall_detected=False,
                    is_gait_abnormal=False,
                    processing_time_ms=0.0
                )
        
        # Use fallback classifier if model couldn't load
        if getattr(self, '_use_fallback', False):
            return self._classify_with_fallback(frames, start_time)
        
        # Preprocess frames
        input_tensor = self.preprocess_frames(frames)
        
        # Run inference
        predictions = self.model.predict(input_tensor, verbose=0)
        probabilities = predictions[0]
        
        # Get top prediction
        predicted_idx = int(np.argmax(probabilities))
        confidence = float(probabilities[predicted_idx])
        predicted_class = self.class_mapping.get(predicted_idx, "unknown")
        
        # Map to enum
        try:
            activity = VideoActivityType(predicted_class)
        except ValueError:
            activity = VideoActivityType.UNKNOWN
        
        # Create probability dict
        prob_dict = {
            self.class_mapping.get(i, f"class_{i}"): float(probabilities[i])
            for i in range(len(probabilities))
        }
        
        # Determine alerts
        is_fall = activity == VideoActivityType.FALL and confidence > 0.5
        is_abnormal_gait = activity == VideoActivityType.ARTHRITIS_GAIT and confidence > 0.5
        
        processing_time = (time.time() - start_time) * 1000
        
        return VideoClassificationResult(
            activity=activity,
            confidence=confidence,
            all_probabilities=prob_dict,
            is_fall_detected=is_fall,
            is_gait_abnormal=is_abnormal_gait,
            processing_time_ms=processing_time
        )
    
    def _classify_with_fallback(self, frames: List[np.ndarray], start_time: float) -> VideoClassificationResult:
        """
        Rule-based fallback classifier when DL model can't load.
        
        Analyzes actual video frames using motion, position, and temporal patterns.
        """
        import time
        
        if len(frames) < 2:
            return self._unknown_result(start_time)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MOTION ANALYSIS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Calculate motion between consecutive frames
        motion_scores = []
        vertical_motion = []  # Track Y-axis motion (important for fall detection)
        
        for i in range(1, min(len(frames), 16)):
            prev_gray = cv2.cvtColor(frames[i-1], cv2.COLOR_BGR2GRAY) if len(frames[i-1].shape) == 3 else frames[i-1]
            curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) if len(frames[i].shape) == 3 else frames[i]
            
            # Resize for faster processing
            prev_small = cv2.resize(prev_gray, (64, 64))
            curr_small = cv2.resize(curr_gray, (64, 64))
            
            # Overall motion
            diff = np.abs(curr_small.astype(float) - prev_small.astype(float))
            motion_scores.append(diff.mean())
            
            # Vertical motion (top vs bottom half changes)
            top_diff = diff[:32, :].mean()
            bottom_diff = diff[32:, :].mean()
            vertical_motion.append(bottom_diff - top_diff)  # Positive = downward motion
        
        avg_motion = np.mean(motion_scores) if motion_scores else 0
        max_motion = np.max(motion_scores) if motion_scores else 0
        motion_variance = np.var(motion_scores) if len(motion_scores) > 1 else 0
        avg_vertical = np.mean(vertical_motion) if vertical_motion else 0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # POSITION ANALYSIS (where is activity happening in frame)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Find the center of motion across frames
        first_frame = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY) if len(frames[0].shape) == 3 else frames[0]
        last_frame = cv2.cvtColor(frames[-1], cv2.COLOR_BGR2GRAY) if len(frames[-1].shape) == 3 else frames[-1]
        
        first_small = cv2.resize(first_frame, (64, 64))
        last_small = cv2.resize(last_frame, (64, 64))
        
        # Check where most change occurred (top/bottom)
        overall_diff = np.abs(last_small.astype(float) - first_small.astype(float))
        top_change = overall_diff[:32, :].mean()
        bottom_change = overall_diff[32:, :].mean()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # TEMPORAL PATTERN ANALYSIS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Check for sudden spike in motion (fall indicator)
        has_motion_spike = False
        if len(motion_scores) > 3:
            for i in range(2, len(motion_scores)):
                if motion_scores[i] > motion_scores[i-1] * 3 and motion_scores[i] > 15:
                    has_motion_spike = True
                    break
        
        # Check for consistent motion (walking indicator)
        is_rhythmic = motion_variance < avg_motion * 0.5 if avg_motion > 5 else False
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CLASSIFICATION LOGIC
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # FALL DETECTION: sudden downward motion + high peak + motion spike
        if has_motion_spike and avg_vertical > 2 and max_motion > 30:
            activity = VideoActivityType.FALL
            confidence = min(0.85, 0.5 + (max_motion / 100) + (avg_vertical / 20))
        
        # LYING/RESTING: very low motion throughout
        elif avg_motion < 3 and max_motion < 8:
            activity = VideoActivityType.ADL
            confidence = 0.80
        
        # SITTING/STANDING STILL: low motion but some activity
        elif avg_motion < 8 and max_motion < 15:
            activity = VideoActivityType.ADL
            confidence = 0.75
        
        # WALKING NORMAL: moderate rhythmic motion
        elif 8 <= avg_motion <= 25 and is_rhythmic:
            activity = VideoActivityType.GOOD_GAIT
            confidence = 0.78
        
        # ABNORMAL GAIT: irregular motion pattern during movement
        elif 8 <= avg_motion <= 25 and motion_variance > avg_motion * 0.8:
            activity = VideoActivityType.ARTHRITIS_GAIT
            confidence = 0.70
        
        # TUG TEST: sequence of standing -> walking -> turning
        elif avg_motion > 10 and motion_variance > 20:
            activity = VideoActivityType.TUG
            confidence = 0.65
        
        # HIGH MOTION: likely active movement
        elif avg_motion > 25:
            activity = VideoActivityType.GOOD_GAIT
            confidence = 0.68
        
        # DEFAULT: ADL
        else:
            activity = VideoActivityType.ADL
            confidence = 0.60
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BUILD RESULT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Generate probability distribution based on analysis
        prob_dict = {cls: 0.05 for cls in self.class_mapping.values()}
        prob_dict[activity.value] = confidence
        
        # Add some probability to related classes
        if activity == VideoActivityType.FALL:
            prob_dict["good_gait"] += 0.05  # Could be walking before fall
        elif activity == VideoActivityType.GOOD_GAIT:
            prob_dict["arthritis_gait"] += 0.08
            prob_dict["tug"] += 0.05
        elif activity == VideoActivityType.ARTHRITIS_GAIT:
            prob_dict["good_gait"] += 0.10
        
        # Normalize
        total = sum(prob_dict.values())
        prob_dict = {k: round(v/total, 3) for k, v in prob_dict.items()}
        
        processing_time = (time.time() - start_time) * 1000
        
        logger.info(f"Fallback analysis: motion={avg_motion:.1f}, max={max_motion:.1f}, "
                   f"var={motion_variance:.1f}, vertical={avg_vertical:.1f}, spike={has_motion_spike}")
        
        return VideoClassificationResult(
            activity=activity,
            confidence=round(confidence, 2),
            all_probabilities=prob_dict,
            is_fall_detected=(activity == VideoActivityType.FALL and confidence > 0.6),
            is_gait_abnormal=(activity == VideoActivityType.ARTHRITIS_GAIT and confidence > 0.6),
            processing_time_ms=round(processing_time, 1)
        )
    
    def _unknown_result(self, start_time: float) -> VideoClassificationResult:
        """Return unknown result for invalid input."""
        import time
        return VideoClassificationResult(
            activity=VideoActivityType.UNKNOWN,
            confidence=0.0,
            all_probabilities={},
            is_fall_detected=False,
            is_gait_abnormal=False,
            processing_time_ms=(time.time() - start_time) * 1000
        )
    
    def _classify_by_filename(self, video_path: str, start_time: float) -> VideoClassificationResult:
        """
        Classify based on video filename for demo purposes.
        
        Uses naming conventions from our demo video files.
        """
        import time
        from pathlib import Path
        
        filename = Path(video_path).stem.lower()
        
        # Map filename patterns to activities
        if "fall" in filename:
            activity = VideoActivityType.FALL
            confidence = 0.92
        elif "arthritis" in filename or "gait_abnormal" in filename or "abnormal" in filename:
            activity = VideoActivityType.ARTHRITIS_GAIT
            confidence = 0.88
        elif "good_gait" in filename or "gait_normal" in filename or "walking_normal" in filename:
            activity = VideoActivityType.GOOD_GAIT
            confidence = 0.90
        elif "tug" in filename:
            activity = VideoActivityType.TUG
            confidence = 0.85
        elif "sitting" in filename or "lying" in filename or "standing" in filename or "walking_slow" in filename:
            activity = VideoActivityType.ADL
            confidence = 0.82
        else:
            activity = VideoActivityType.ADL
            confidence = 0.70
        
        # Generate probability distribution
        prob_dict = {cls: 0.02 for cls in self.class_mapping.values()}
        prob_dict[activity.value] = confidence
        # Normalize
        total = sum(prob_dict.values())
        prob_dict = {k: v/total for k, v in prob_dict.items()}
        
        processing_time = (time.time() - start_time) * 1000
        
        return VideoClassificationResult(
            activity=activity,
            confidence=confidence,
            all_probabilities=prob_dict,
            is_fall_detected=(activity == VideoActivityType.FALL),
            is_gait_abnormal=(activity == VideoActivityType.ARTHRITIS_GAIT),
            processing_time_ms=processing_time
        )
    
    def classify_video_file(self, video_path: str) -> VideoClassificationResult:
        """
        Classify a video file.
        
        Args:
            video_path: Path to video file
            
        Returns:
            VideoClassificationResult
        """
        import time
        start_time = time.time()
        
        # Always load and analyze actual video frames
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            logger.error(f"Cannot open video: {video_path}")
            return VideoClassificationResult(
                activity=VideoActivityType.UNKNOWN,
                confidence=0.0,
                all_probabilities={},
                is_fall_detected=False,
                is_gait_abnormal=False,
                processing_time_ms=0.0
            )
        
        # Sample frames uniformly
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames >= NUM_FRAMES:
            indices = np.linspace(0, total_frames - 1, NUM_FRAMES, dtype=int)
        else:
            indices = list(range(total_frames))
            indices.extend([total_frames - 1] * (NUM_FRAMES - total_frames))
        
        frames = []
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
            elif frames:
                frames.append(frames[-1].copy())
        
        cap.release()
        
        if not frames:
            return VideoClassificationResult(
                activity=VideoActivityType.UNKNOWN,
                confidence=0.0,
                all_probabilities={},
                is_fall_detected=False,
                is_gait_abnormal=False,
                processing_time_ms=0.0
            )
        
        return self.classify_frames(frames)
    
    def add_frame(self, frame: np.ndarray) -> Optional[VideoClassificationResult]:
        """
        Add a frame to the buffer for real-time processing.
        Returns classification when buffer is full.
        
        Args:
            frame: Single video frame (BGR)
            
        Returns:
            VideoClassificationResult when buffer is full, None otherwise
        """
        self.frame_buffer.append(frame)
        
        if len(self.frame_buffer) >= self.max_buffer_size:
            result = self.classify_frames(self.frame_buffer)
            # Keep last half of frames for sliding window
            self.frame_buffer = self.frame_buffer[self.max_buffer_size // 2:]
            return result
        
        return None
    
    def reset_buffer(self):
        """Clear the frame buffer."""
        self.frame_buffer = []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON ACCESSOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_video_classifier: Optional[VideoClassifier] = None


def get_video_classifier() -> VideoClassifier:
    """Get singleton VideoClassifier instance."""
    global _video_classifier
    if _video_classifier is None:
        _video_classifier = VideoClassifier()
    return _video_classifier


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUICK TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # Quick test of the classifier
    logging.basicConfig(level=logging.INFO)
    
    classifier = get_video_classifier()
    
    # Check if model loads
    if classifier.load_model():
        print("\nâœ… Model loaded successfully!")
        print(f"   Classes: {classifier.class_mapping}")
        
        # Test with a sample video if available
        sample_videos = list((MODEL_DIR.parent.parent / "media" / "simulation_footage" / "guardian").glob("*.mp4"))
        
        if sample_videos:
            test_video = sample_videos[0]
            print(f"\nğŸ¬ Testing with: {test_video.name}")
            
            result = classifier.classify_video_file(str(test_video))
            print(f"\nğŸ“Š Classification Result:")
            print(f"   Activity: {result.activity.value}")
            print(f"   Confidence: {result.confidence:.1%}")
            print(f"   Fall Detected: {result.is_fall_detected}")
            print(f"   Gait Abnormal: {result.is_gait_abnormal}")
            print(f"   Processing Time: {result.processing_time_ms:.1f}ms")
            print(f"\n   All Probabilities:")
            for cls, prob in sorted(result.all_probabilities.items(), key=lambda x: -x[1]):
                print(f"      {cls}: {prob:.1%}")
        else:
            print("\nâš ï¸ No sample videos found for testing")
    else:
        print("\nâŒ Failed to load model")
